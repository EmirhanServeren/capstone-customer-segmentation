{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMER SEGMENTATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, declaring necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json     # for reading the key inside the json formatted file\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyodbc   # for connecting database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Connecting to Database\n",
    "\n",
    "Pyodbc library handles the connection between Jupyter notebook and MS SQL Server. SQL Server's key is hidden inside the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('log.json')\n",
    "sql_key = json.load(f)     # returns JSON object as a dictionary\n",
    "\n",
    "cnxn = pyodbc.connect(sql_key['key'])     # establish a connection\n",
    "crsr = cnxn.cursor()                      # cursor enables to send command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sending Queries \n",
    "\n",
    "Queries are sent with respect to the decisions given on analytics phase. G and T type company data are retrieved separately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For G (Şahıs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_query= \"\"\"SELECT MUSTERI_ID, ID, CEK_NO, CEK_TUTAR, VADE_GUN, SIRKET_TURU, BK_LIMIT, BK_RISK,\n",
    "            BK_GECIKMEHESAP, BK_GECIKMEBAKIYE\n",
    "            FROM dbo.dataset\n",
    "            WHERE SIRKET_TURU LIKE 'G' \"\"\"\n",
    "\n",
    "g_company_type_df = pd.read_sql(gk_query, cnxn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For T (Tüzel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_query= \"\"\"SELECT MUSTERI_ID, ID, CEK_NO, CEK_TUTAR, VADE_GUN, SIRKET_TURU, TK_NAKDILIMIT, TK_NAKDIRISK, TK_GAYRINAKDILIMIT, TK_GAYRINAKDIRISK, TK_GECIKMEHESAP, TK_GECIKMEBAKIYE FROM dbo.dataset WHERE SIRKET_TURU LIKE 'T' \"\"\"\n",
    "t_company_type_df = pd.read_sql(tk_query, cnxn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING\n",
    "\n",
    "The ML is going to be implemented to segment customer portfolio into clusters based on their risks. Firstly, the customer portfolio divided into two: T type and G type customers. Due to the differences between their attributes, this was inevitable step to be done. Also, it is crucial to define type of machine learning. Due to the attributes, it will be unsupervised learning. As we observe, all attributes will be used are features. For providing accurate solution, we obtained that classification of the customer portfolio is a must requirements. In this sense, **K-means** is going to implemented.\n",
    "\n",
    "Next, the datasets will be prepared for the clustering. The feature extraction and data scaling must be done before putting data into model. Also, data will be divided into two as training and test for ML. Moreover, cluster amount is going to be obtained by using Elbow Method. After this, we are going to put the training data into model.\n",
    "\n",
    "After these works, the accuracy of K-means model must be found out. Also, other appropriate unsupervised ML modelling techniques are going to be compared. The most fitting model's results are going to be saved into the database. The database is going to be integrated our data-oriented web application for strong Business Intelligence presentation.\n",
    "\n",
    "To conclude, our steps are;\n",
    "\n",
    "*   Feature Extraction\n",
    "*   Scaling Data\n",
    "*   Training/Test Set Division\n",
    "*   Elbow Method\n",
    "*   Accuracy Calculation\n",
    "*   Comparing K-means with Other Models\n",
    "*   Saving Results into Database\n",
    "*   Presenting Results via Streamlit Web-App"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... to be done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(df)      # df is not exist yet, volatile to change later\n",
    "scaled_data = scaler.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Elbow Method\n",
    "\n",
    "We aim to maximize the efficiency of segmentation while minimizing the number of clusters. In this sense, Elbow Method is crucial concept to satisfy this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing scikit-learning library for the K-means model\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(df, maximum_K):\n",
    "    clusters_centers = []   # appending inertia value coming from the model\n",
    "    k_values = []           # putting k values from 0 to maximum K\n",
    "\n",
    "    for k in range(1, maximum_K):\n",
    "        kmeans_model = KMeans(n_clusters = k)\n",
    "        kmeans_model.fit(df)\n",
    "\n",
    "        clusters_centers.append(kmeans_model.inertia_)\n",
    "        k_values.append(k)\n",
    "\n",
    "    return clusters_centers, k_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note About **Inertia**:\n",
    "\n",
    "Inertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters (K) *-Codeacademy*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, illustrating an elbow method for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elbow_plot(clusters_centers, k_values):\n",
    "    figure = plt.subplots(figsize = (12, 6))\n",
    "    plt.plot(k_values, clusters_centers, 'o-', color = 'blue')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Cluster Inertia\")\n",
    "    plt.title(\"Elbow Plot of Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_centers, k_values = find_optimal_clusters(df, 16)\n",
    "generate_elbow_plot(clusters_centers, k_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
